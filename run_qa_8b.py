"""from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch

model_path = "./qa_ckpts/MERGED/llama38b-LoRD-VI-turthful_qa/"

# ä¿®æ­£è®¡ç®—ç²¾åº¦ä¸ºfloat16
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # å¼ºåˆ¶è®¡ç®—ä¸è¾“å…¥ç±»å‹ä¸€è‡´
    bnb_4bit_quant_type="nf4"
)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,  # æ›¿æ¢æ—§ç‰ˆload_in_4bitå‚æ•°
    device_map="auto",
    vocab_size=128256  # æ˜¾å¼æŒ‡å®šè¯æ±‡é‡
)

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",  # ç¡®ä¿ç”Ÿæˆæ–¹å‘ä¸€è‡´
    use_fast=False        # å…³é—­å¿«é€Ÿæ¨¡å¼é¿å…å…¼å®¹æ€§é—®é¢˜
)
# tokenizer.pad_token = tokenizer.eos_token  # æ˜¾å¼è®¾ç½®å¡«å……ç¬¦ 

# è®¡ç®—éœ€æ·»åŠ çš„ Token æ•°é‡
# target_vocab_size = 128256
# num_tokens_to_add = target_vocab_size - tokenizer.vocab_size
# print(num_tokens_to_add)

# # ç”Ÿæˆå ä½ç¬¦ Tokenï¼ˆå¦‚ reserved_0 åˆ° reserved_255ï¼‰
# new_tokens = [f"reserved_{i}" for i in range(num_tokens_to_add)]

# # æ·»åŠ è‡³ Tokenizer
# tokenizer.add_tokens(new_tokens)
# print(f"æ‰©å±•åè¯æ±‡é‡: {len(tokenizer)}")  # åº”ä¸º 128,256

# print(f"Tokenizerè¯æ±‡é‡: {tokenizer.vocab_size}") 
# print(f"æ¨¡å‹åµŒå…¥å±‚å¤§å°: {model.get_input_embeddings().weight.shape[0]}")

# # éªŒè¯ Tokenizer åŠŸèƒ½
# sample = "æ‰©å±•åçš„æµ‹è¯•æ–‡æœ¬"
# input_ids = tokenizer.encode(sample, add_special_tokens=False)
# print("æœ€å¤§ç´¢å¼•å€¼:", max(input_ids))  # åº” < 128,256

# # æ£€æŸ¥ç‰¹æ®Š Token æ˜¯å¦ä¿ç•™
# print("[PAD] ID:", tokenizer.pad_token_id)  # åº”ä¸åŸå€¼ä¸€è‡´

while True:
    message = input("ğŸ‘¤User: ")
    if message == "q":
        break
    inputs = tokenizer(message, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.3,              # é™ä½éšæœºæ€§
        top_p=0.85,                   # æ”¶ç´§é‡‡æ ·èŒƒå›´
        repetition_penalty=2.0,        
        typical_p = 0.95,
        do_sample=True,               
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        # stopping_criteria=[StopOnTokens()]  # æ·»åŠ ç»ˆæ­¢æ¡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰
    )
    print("ğŸ¤–AI: ", end='')
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    torch.cuda.empty_cache()

print("Stop Chat.")"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
import torch

model_path = "./qa_ckpts/MERGED/llama38b-kd-ceval/"
# model_path = "./qa_ckpts/MERGED/llama38b-kd-truthful_qa/"
#model_path = "./qa_ckpts/MERGED/llama38b-vanilla-truthful_qa/"

# é…ç½®4-bité‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    # quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    # padding_side="left",
    # use_fast=False
)

# æ£€æŸ¥å¹¶æ‰©å±•è¯æ±‡è¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
target_vocab_size = 128256
current_vocab_size = len(tokenizer)

if current_vocab_size < target_vocab_size:
    print(f"å½“å‰è¯æ±‡é‡: {current_vocab_size}, ç›®æ ‡è¯æ±‡é‡: {target_vocab_size}")
    num_tokens_to_add = target_vocab_size - current_vocab_size
    new_tokens = [f"<extra_token_{i}>" for i in range(num_tokens_to_add)]
    tokenizer.add_tokens(new_tokens)
    model.resize_token_embeddings(len(tokenizer))
    print(f"å·²æ·»åŠ  {num_tokens_to_add} ä¸ªæ–°tokenï¼Œè¯æ±‡é‡ç°åœ¨ä¸º: {len(tokenizer)}")

# ç¡®ä¿ç‰¹æ®Štokenè®¾ç½®æ­£ç¡®
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if tokenizer.bos_token is None:
    tokenizer.add_special_tokens({'bos_token': '<s>'})

print("\n===== æ¨¡å‹åŠ è½½å®Œæˆ =====")
print(f"Tokenizerè¯æ±‡é‡: {len(tokenizer)}")
print(f"æ¨¡å‹åµŒå…¥å±‚å¤§å°: {model.get_input_embeddings().weight.shape[0]}")
print(f"Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
print("========================\n")

print(f"ğŸ¤–AI: Hello! I'm an AI assistant. Ask me any question you have!")

import ftfy

def fix_mojibake(text: str) -> str:
    """
    ä¿®å¤å› ç¼–ç æ··æ·†å¯¼è‡´çš„UTF-8ä¹±ç ï¼ˆå¦‚Latin-1è¯¯è§£ç ï¼‰
    æ”¯æŒå¤šå±‚ç¼–ç é”™è¯¯ä¿®å¤ï¼ˆå¦‚ "ÃƒÆ’Ã‚" â†’ "A"ï¼‰
    """
    return ftfy.fix_text(text)

while True:
    try:
        message = input("ğŸ‘¤User: ")
        if message.lower() == "q":
            break

        # ===== æ–°å¢çº¦æŸæŒ‡ä»¤ =====
        constraint_rules = [
            "ã€Mandatory Constraintsã€‘",  # Using "Mandatory" as in [3](@ref)'s "Mandatory clause"
            "1. Responses must not exceed 5 sentences",  # "Constraints" aligned with financial constraint terminology [1](@ref)
            "2. Output only core facts; explain the reasons in 3 sentences.",  # "Core facts" maintains precision requirement
            "3. For multi-step reasoning questions, provide the final conclusion directly",  # "Directly" corresponds to operative constraint principle [2](@ref)
            "4. Answer the question by Chinese"
        ]
        constraint_instruction = "\n".join(constraint_rules)
        # ======================
        
        # ä½¿ç”¨å¯¹è¯æ ¼å¼ï¼ˆæ ¹æ®æ¨¡å‹è®­ç»ƒæ ¼å¼è°ƒæ•´ï¼‰
        formatted_input = f"### System: {constraint_instruction}\n### Human: å›ç­”æˆ‘çš„é—®é¢˜: {message}\n### Assistant:"
        
        # Tokenizeè¾“å…¥
        inputs = tokenizer(
            formatted_input,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
            return_attention_mask=True
        ).to(model.device)
        
        # ç”Ÿæˆå“åº”
        outputs = model.generate(
            **inputs,  # åŒ…å«input_idså’Œattention_mask
            max_new_tokens=200,
            temperature=0.3,
            top_p=0.3,
            repetition_penalty=1.3,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
        
        # æå–å¹¶è§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        new_tokens = outputs[0][inputs.input_ids.shape[1]:]
        response = tokenizer.decode(
            new_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        ).strip()
        
        # å¤„ç†ç©ºå“åº”
        if not response:
            response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå“åº”"

        # res = ''
        # for i in range(len(response)):
        #     if response[i] == "\\":
        #         byte_data = response[i:i+12].encode('latin1').decode('unicode_escape').encode('latin1')
        #         decoded_text = byte_data.decode('utf-8')
        #         res += decoded_text
        #         i += 8
        #     else:
        #         res += response[i]
        print(response)
        print(f"ğŸ¤–AI: {fix_mojibake(response)}")
        
    except KeyboardInterrupt:
        print("\né€€å‡ºå¯¹è¯...")
        break
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
    finally:
        torch.cuda.empty_cache()

print("å¯¹è¯å·²ç»“æŸ")